# ----------------------------------
# Python API Configuration
# ----------------------------------

# For each service, choose "local" to use a local model file, or "api" to use a Hugging Face 'transformers' model.
TRANSLATION_PROVIDER=local
SUMMARIZATION_PROVIDER=local

# --- Local Model Paths (used if provider is 'local') ---
# Use the full, absolute path to your downloaded GGUF model files.
TRANSLATE_MODEL_PATH="/home/anggit/models/sanskrit-qwen-7b-translate-q4-ks.gguf"
SUMMARIZE_MODEL_PATH="/home/anggit/models/tensorflow-towerbase-7B-summarize.gguf"


# --- API Model Names (used if provider is 'api') ---
# These are default models from Hugging Face that will be downloaded and cached.
API_TRANSLATE_MODEL_NAME="Helsinki-NLP/opus-mt-en-es"
API_SUMMARIZE_MODEL_NAME="facebook/bart-large-cnn"


# --- Whisper Model (always local with faster-whisper) ---
# Options: "tiny", "base", "small", "medium", "large-v3", "distil-large-v3"
WHISPER_MODEL_NAME="base"
